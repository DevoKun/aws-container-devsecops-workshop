{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview In this workshop, you will learn how to leverage AWS development tools and open-source projects to integrate automated security testing into a CI/CD pipeline. You will learn about a variety of patterns for integrating security-centric release control into AWS CodePipeline. Additionally, you will learn how to add feedback loops and fix common security vulnerabilities in your container-based applications. Level : Advanced Duration : 2 - 3 hours Prerequisites : AWS Account, Admin IAM User CSF Functions : Prevent, Detect CAF Components : Preventative, Detective AWS Services : Amazon CloudWatch , AWS CodeCommit , AWS CodeBuild , AWS CodePipeline , Amazon ECR , AWS Lambda , and AWS Security Hub Open Source Projects : Hadolint , Trufflehog , and Anchore Scenario Your company has just kicked off a new DevSecOps initiative in an effort to improve the security of critical applications by embedding security in every part of the software development lifecycle. You are part of a DevOps team tasked with integrating security testing into a rudimentary pipeline for building and releasing container images. Your initial tasks include adding Dockerfile linting, secrets scanning, and vulnerability scanning. The decision has been made to evaluate and make use of open source projects with the possibility of moving to a commercial offering based on how well the requirements are met. Architecture For this workshop you will start with a basic CI/CD pipeline that is triggered on Pull Requests and builds and pushes a container image to an Amazon ECR repository. As you work through the tasks in your latest sprint you'll end up with the CI/CD pipeline as shown below. It will include stages within your AWS CodePipeline for linting Dockerfiles, scanning for secrets, and scanning for vulnerabilities including an integration with AWS Security Hub. In addition you will be using a combination of Amazon CloudWatch Event Rules and AWS Lambda Functions to create feedback loops for each stage of security testing. This will allow your developers to quickly fix and iterate on their code which will lead to faster and more secure deliveries. Presentation deck Workshop Presentation Deck Region Please use the us-east-2 (Ohio) region for this workshop. This workshop is broken up into five modules. Please click the bottom right button to proceed to the environment setup.","title":"Overview"},{"location":"#overview","text":"In this workshop, you will learn how to leverage AWS development tools and open-source projects to integrate automated security testing into a CI/CD pipeline. You will learn about a variety of patterns for integrating security-centric release control into AWS CodePipeline. Additionally, you will learn how to add feedback loops and fix common security vulnerabilities in your container-based applications. Level : Advanced Duration : 2 - 3 hours Prerequisites : AWS Account, Admin IAM User CSF Functions : Prevent, Detect CAF Components : Preventative, Detective AWS Services : Amazon CloudWatch , AWS CodeCommit , AWS CodeBuild , AWS CodePipeline , Amazon ECR , AWS Lambda , and AWS Security Hub Open Source Projects : Hadolint , Trufflehog , and Anchore","title":"Overview"},{"location":"#scenario","text":"Your company has just kicked off a new DevSecOps initiative in an effort to improve the security of critical applications by embedding security in every part of the software development lifecycle. You are part of a DevOps team tasked with integrating security testing into a rudimentary pipeline for building and releasing container images. Your initial tasks include adding Dockerfile linting, secrets scanning, and vulnerability scanning. The decision has been made to evaluate and make use of open source projects with the possibility of moving to a commercial offering based on how well the requirements are met.","title":"Scenario"},{"location":"#architecture","text":"For this workshop you will start with a basic CI/CD pipeline that is triggered on Pull Requests and builds and pushes a container image to an Amazon ECR repository. As you work through the tasks in your latest sprint you'll end up with the CI/CD pipeline as shown below. It will include stages within your AWS CodePipeline for linting Dockerfiles, scanning for secrets, and scanning for vulnerabilities including an integration with AWS Security Hub. In addition you will be using a combination of Amazon CloudWatch Event Rules and AWS Lambda Functions to create feedback loops for each stage of security testing. This will allow your developers to quickly fix and iterate on their code which will lead to faster and more secure deliveries.","title":"Architecture"},{"location":"#presentation-deck","text":"Workshop Presentation Deck","title":"Presentation deck"},{"location":"#region","text":"Please use the us-east-2 (Ohio) region for this workshop. This workshop is broken up into five modules. Please click the bottom right button to proceed to the environment setup.","title":"Region"},{"location":"00-env-setup/","text":"Module 0 Environment Setup Time : 15 minutes In the first module you will be configuring the initial pipeline and setting up the Anchore service which you will be integrating into the pipeline later on in this workshop. This module requires you to run an AWS CloudFormation template, which will automate the creation of the pipeline and Anchore service. You will then walk through each stage and manually configure the security testing. Setup your environment To setup your environment please expand one of the following dropdown sections (depending on how you're doing this workshop) and follow the instructions: Click here if you're at an AWS event where the Event Engine is being used Navigate to the Event Engine dashboard Enter the team hash code that was distributed to you by the instructors. Click AWS Console . The CloudFormation template for this round has already been prerun. Click here if you're running this individually in your own AWS Account The CloudFormation template you run will create the Anchore vulnerability scanning service as well as the initial pipeline. Before you deploy the CloudFormation template feel free to view it here . Region Deploy US East 2 (Ohio) Click the Deploy to AWS button above. This will automatically take you to the console to run the template. The Specify an Amazon S3 template URL is already selected and the template URL is automatically added. Click Next . On the Specify Details click Next . On the Options click Next (leave everything on this page as the default). Finally, on the Review section acknowledge that the template will create IAM roles and CAPABILITY_AUTO_EXPAND and click Create . What is CAPABILITY_AUTO_EXPAND? Some templates contain macros. Macros perform custom processing on templates; this can include simple actions like find-and-replace operations, all the way to extensive transformations of entire templates. Because of this, users typically create a change set from the processed template, so that they can review the changes resulting from the macros before actually creating the stack. If your stack template contains one or more macros, and you choose to create a stack directly from the processed template, without first reviewing the resulting changes in a change set, you must acknowledge this capability. This will bring you back to the CloudFormation console. You can refresh the page to see the stack starting to create. Before moving on, make sure the stack is in a CREATE_COMPLETE status. This stack takes ~9 minutes. Browse to your Cloud9 IDE You will be doing the majority of the workshop using the AWS Command Line Interface (CLI) within AWS Cloud9 , a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. Open the AWS Cloud9 console (us-east-2) Click Open IDE in the container-devsecops-wksp-ide environment. This will take you to your IDE in a new tab. Always keep this tab open Where is my terminal window? If a new terminal session was not opened automatically on creation, go to 'Window' -> 'New Terminal' Your main working directory will be ~/environment Clone repositories Go back to your Cloud9 IDE Setup your git credentials and clone the repo that contains all the configurations for your pipeline: git config --global credential.helper '!aws codecommit credential-helper $@' git config --global credential.UseHttpPath true git clone https://git-codecommit.us-east-2.amazonaws.com/v1/repos/container-devsecops-wksp-config configurations git clone https://git-codecommit.us-east-2.amazonaws.com/v1/repos/container-devsecops-wksp-app sample-application echo 'Cloned repositories.' What is the aws codecommit credential-helper? The credential-helper utility is not designed to be called directly from the AWS CLI. Instead it is intended to be used as a parameter with the git config command to set up your local computer. It enables Git to use HTTPS and a cryptographically signed version of your IAM user credentials or Amazon EC2 instance role whenever Git needs to authenticate with AWS to interact with CodeCommit repositories. Enable AWS Security Hub You will be using AWS Security Hub to manage your container image vulnerabilities. Enable Security Hub aws securityhub enable-security-hub Pipeline Architecture You can browse to AWS CodePipeline to view your current pipeline. All the stages are there but they have not been properly configured. Below is the current architecture of your pipeline. After you have successfully setup your environment, you can proceed to the next module.","title":"Module 0: Environment Setup"},{"location":"00-env-setup/#module-0-environment-setup","text":"Time : 15 minutes In the first module you will be configuring the initial pipeline and setting up the Anchore service which you will be integrating into the pipeline later on in this workshop. This module requires you to run an AWS CloudFormation template, which will automate the creation of the pipeline and Anchore service. You will then walk through each stage and manually configure the security testing.","title":"Module 0 Environment Setup"},{"location":"00-env-setup/#setup-your-environment","text":"To setup your environment please expand one of the following dropdown sections (depending on how you're doing this workshop) and follow the instructions: Click here if you're at an AWS event where the Event Engine is being used Navigate to the Event Engine dashboard Enter the team hash code that was distributed to you by the instructors. Click AWS Console . The CloudFormation template for this round has already been prerun. Click here if you're running this individually in your own AWS Account The CloudFormation template you run will create the Anchore vulnerability scanning service as well as the initial pipeline. Before you deploy the CloudFormation template feel free to view it here . Region Deploy US East 2 (Ohio) Click the Deploy to AWS button above. This will automatically take you to the console to run the template. The Specify an Amazon S3 template URL is already selected and the template URL is automatically added. Click Next . On the Specify Details click Next . On the Options click Next (leave everything on this page as the default). Finally, on the Review section acknowledge that the template will create IAM roles and CAPABILITY_AUTO_EXPAND and click Create . What is CAPABILITY_AUTO_EXPAND? Some templates contain macros. Macros perform custom processing on templates; this can include simple actions like find-and-replace operations, all the way to extensive transformations of entire templates. Because of this, users typically create a change set from the processed template, so that they can review the changes resulting from the macros before actually creating the stack. If your stack template contains one or more macros, and you choose to create a stack directly from the processed template, without first reviewing the resulting changes in a change set, you must acknowledge this capability. This will bring you back to the CloudFormation console. You can refresh the page to see the stack starting to create. Before moving on, make sure the stack is in a CREATE_COMPLETE status. This stack takes ~9 minutes.","title":"Setup your environment"},{"location":"00-env-setup/#browse-to-your-cloud9-ide","text":"You will be doing the majority of the workshop using the AWS Command Line Interface (CLI) within AWS Cloud9 , a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. Open the AWS Cloud9 console (us-east-2) Click Open IDE in the container-devsecops-wksp-ide environment. This will take you to your IDE in a new tab. Always keep this tab open Where is my terminal window? If a new terminal session was not opened automatically on creation, go to 'Window' -> 'New Terminal' Your main working directory will be ~/environment","title":"Browse to your Cloud9 IDE"},{"location":"00-env-setup/#clone-repositories","text":"Go back to your Cloud9 IDE Setup your git credentials and clone the repo that contains all the configurations for your pipeline: git config --global credential.helper '!aws codecommit credential-helper $@' git config --global credential.UseHttpPath true git clone https://git-codecommit.us-east-2.amazonaws.com/v1/repos/container-devsecops-wksp-config configurations git clone https://git-codecommit.us-east-2.amazonaws.com/v1/repos/container-devsecops-wksp-app sample-application echo 'Cloned repositories.' What is the aws codecommit credential-helper? The credential-helper utility is not designed to be called directly from the AWS CLI. Instead it is intended to be used as a parameter with the git config command to set up your local computer. It enables Git to use HTTPS and a cryptographically signed version of your IAM user credentials or Amazon EC2 instance role whenever Git needs to authenticate with AWS to interact with CodeCommit repositories.","title":"Clone repositories"},{"location":"00-env-setup/#enable-aws-security-hub","text":"You will be using AWS Security Hub to manage your container image vulnerabilities. Enable Security Hub aws securityhub enable-security-hub","title":"Enable AWS Security Hub"},{"location":"00-env-setup/#pipeline-architecture","text":"You can browse to AWS CodePipeline to view your current pipeline. All the stages are there but they have not been properly configured. Below is the current architecture of your pipeline. After you have successfully setup your environment, you can proceed to the next module.","title":"Pipeline Architecture"},{"location":"01-linting/","text":"Module 1 Add a Dockerfile linting stage Time : 10 minutes Now that you have your initial pipeline setup, it is time to start integrating security testing. The first stage you'll add is for doing linting of Dockerfiles to help you build best practice Docker images. For linting you'll be leveraging Hadolint , which is a popular open source project for linting Dockerfiles and validating inline bash. The linter parses the Dockerfile into an AST and performs rules on top of the AST. The rules aren't all security specific but they have good coverage across best practices. View your CodeBuild Project For each AWS CodePipeline stage you'll be using AWS CodeBuild , which is a continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. The CodeBuild project for Dockerfile linting has already been created but hasn't been properly configured. Click here to view your CodeBuild project Create the Build Spec file Each CodeBuild project contains a build specification (build spec) file, which is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build. This is the file where you define the commands for doing Dockerfile linting using Hadolint. Click on your Cloud9 IDE tab. In the left file tree, expand the configurations folder and open buildspec_dockerfile.yml . Review the YAML code below, paste it in the file, and save it. version : 0.2 phases : pre_build : commands : - echo \"Copying hadolint.yml to the application directory\" - cp hadolint.yml $CODEBUILD_SRC_DIR_AppSource/hadolint.yml - echo \"Switching to the application directory\" - cd $CODEBUILD_SRC_DIR_AppSource - echo \"Pulling the hadolint docker image\" - docker pull hadolint/hadolint:v1.16.2 build : commands : - echo \"Build started on $(date)\" - echo \"Scanning with Hadolint...\" - result=`docker run --rm -i -v ${PWD}/hadolint.yml:/.hadolint.yaml hadolint/hadolint:v1.16.2 hadolint -f json - < Dockerfile` post_build : commands : - echo \"Lint Results:\" - echo $result | jq . - aws ssm put-parameter --name \"codebuild-dockerfile-results\" --type \"String\" --value \"$result\" --overwrite - echo Build completed on `date` Add the Hadolint configuration When using Hadolint you can optionally specify a configuration file to ignore certain rules you might not necessary care about as well as specify trusted registries. You can view all the current rules by scrolling down on the Hadolint github project In the left file tree, expand the configurations folder and open hadolint.yml . Paste the YAML below and save the file. ignored : - DL3000 - DL3025 trustedRegistries : - examplecorp.com Pipeline Architecture Below is the current architecture of your pipeline. After you have successfully configured the Dockerfile linting stage, you can proceed to the next module.","title":"Module 1: Dockerfile Linting"},{"location":"01-linting/#module-1-add-a-dockerfile-linting-stage","text":"Time : 10 minutes Now that you have your initial pipeline setup, it is time to start integrating security testing. The first stage you'll add is for doing linting of Dockerfiles to help you build best practice Docker images. For linting you'll be leveraging Hadolint , which is a popular open source project for linting Dockerfiles and validating inline bash. The linter parses the Dockerfile into an AST and performs rules on top of the AST. The rules aren't all security specific but they have good coverage across best practices.","title":"Module 1 Add a Dockerfile linting stage"},{"location":"01-linting/#view-your-codebuild-project","text":"For each AWS CodePipeline stage you'll be using AWS CodeBuild , which is a continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. The CodeBuild project for Dockerfile linting has already been created but hasn't been properly configured. Click here to view your CodeBuild project","title":"View your CodeBuild Project"},{"location":"01-linting/#create-the-build-spec-file","text":"Each CodeBuild project contains a build specification (build spec) file, which is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build. This is the file where you define the commands for doing Dockerfile linting using Hadolint. Click on your Cloud9 IDE tab. In the left file tree, expand the configurations folder and open buildspec_dockerfile.yml . Review the YAML code below, paste it in the file, and save it. version : 0.2 phases : pre_build : commands : - echo \"Copying hadolint.yml to the application directory\" - cp hadolint.yml $CODEBUILD_SRC_DIR_AppSource/hadolint.yml - echo \"Switching to the application directory\" - cd $CODEBUILD_SRC_DIR_AppSource - echo \"Pulling the hadolint docker image\" - docker pull hadolint/hadolint:v1.16.2 build : commands : - echo \"Build started on $(date)\" - echo \"Scanning with Hadolint...\" - result=`docker run --rm -i -v ${PWD}/hadolint.yml:/.hadolint.yaml hadolint/hadolint:v1.16.2 hadolint -f json - < Dockerfile` post_build : commands : - echo \"Lint Results:\" - echo $result | jq . - aws ssm put-parameter --name \"codebuild-dockerfile-results\" --type \"String\" --value \"$result\" --overwrite - echo Build completed on `date`","title":"Create the Build Spec file"},{"location":"01-linting/#add-the-hadolint-configuration","text":"When using Hadolint you can optionally specify a configuration file to ignore certain rules you might not necessary care about as well as specify trusted registries. You can view all the current rules by scrolling down on the Hadolint github project In the left file tree, expand the configurations folder and open hadolint.yml . Paste the YAML below and save the file. ignored : - DL3000 - DL3025 trustedRegistries : - examplecorp.com","title":"Add the Hadolint configuration"},{"location":"01-linting/#pipeline-architecture","text":"Below is the current architecture of your pipeline. After you have successfully configured the Dockerfile linting stage, you can proceed to the next module.","title":"Pipeline Architecture"},{"location":"02-secrets-scanning/","text":"Module 2 Add a secrets scanning stage Time : 10 minutes Next, you need to setup a stage for identifying secrets throughout your code. For this stage you'll be leveraging trufflehog , a popular open source project for finding secrets accidentally committed in repositories. It essentially searches through git repositories for secrets, digging deep into commit history and branches. It identifies secrets by running entropy checks as well as high signal regex checks. Create the Build Spec file Click on your Cloud9 IDE tab. In the left file tree, expand the configurations folder and open buildspec_secrets.yml . Review the YAML code below, paste it in the file, and save. version : 0.2 phases : pre_build : commands : - echo \"Setting CodeCommit Credentials\" - git config --global credential.helper '!aws codecommit credential-helper $@' - git config --global credential.UseHttpPath true - echo \"Copying secrets_config.json to the application directory\" - cp secrets_config.json $CODEBUILD_SRC_DIR_AppSource/secrets_config.json - echo \"Switching to the application directory\" - echo \"Installing truffleHog\" - which pip3 && pip3 --version - which python3 && python3 --version - pip3 install 'truffleHog>=2.1.0,<3.0' build : commands : - echo \"Build started on $(date)\" - echo \"Scanning with truffleHog...\" - trufflehog --regex --rules secrets_config.json --entropy=False \"$APP_REPO_URL\" post_build : commands : - echo \"Build completed on $(date)\" Add the trufflehog regex configuration When using trufflehog you can optionally specify a configuration file that contains custom regex checks. In the left file tree, expand the configurations folder and open secrets_config.json . Paste the JSON below and save the file. { \"Slack Token\" : \"(xox[p|b|o|a]-[0-9]{12}-[0-9]{12}-[0-9]{12}-[a-z0-9]{32})\" , \"RSA private key\" : \"-----BEGIN RSA PRIVATE KEY-----\" , \"SSH (OPENSSH) private key\" : \"-----BEGIN OPENSSH PRIVATE KEY-----\" , \"SSH (DSA) private key\" : \"-----BEGIN DSA PRIVATE KEY-----\" , \"SSH (EC) private key\" : \"-----BEGIN EC PRIVATE KEY-----\" , \"PGP private key block\" : \"-----BEGIN PGP PRIVATE KEY BLOCK-----\" , \"Facebook Oauth\" : \"[f|F][a|A][c|C][e|E][b|B][o|O][o|O][k|K].*['|\\\"][0-9a-f]{32}['|\\\"]\" , \"Twitter Oauth\" : \"[t|T][w|W][i|I][t|T][t|T][e|E][r|R].*['|\\\"][0-9a-zA-Z]{35,44}['|\\\"]\" , \"GitHub\" : \"[g|G][i|I][t|T][h|H][u|U][b|B].*['|\\\"][0-9a-zA-Z]{35,40}['|\\\"]\" , \"Google Oauth\" : \"(\\\"client_secret\\\":\\\"[a-zA-Z0-9-_]{24}\\\")\" , \"AWS API Key\" : \"AKIA[0-9A-Z]{16}\" , \"Heroku API Key\" : \"[h|H][e|E][r|R][o|O][k|K][u|U].*[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12}\" , \"Generic Secret\" : \"[s|S][e|E][c|C][r|R][e|E][t|T].*['|\\\"][0-9a-zA-Z]{32,45}['|\\\"]\" , \"Generic API Key\" : \"[a|A][p|P][i|I][_]?[k|K][e|E][y|Y].*['|\\\"][0-9a-zA-Z]{32,45}['|\\\"]\" , \"Slack Webhook\" : \"https://hooks.slack.com/services/T[a-zA-Z0-9_]{8}/B[a-zA-Z0-9_]{8}/[a-zA-Z0-9_]{24}\" , \"Google (GCP) Service-account\" : \"\\\"type\\\": \\\"service_account\\\"\" , \"Twilio API Key\" : \"SK[a-z0-9]{32}\" , \"Password in URL\" : \"[a-zA-Z]{3,10}://[^/\\\\s:@]{3,20}:[^/\\\\s:@]{3,20}@.{1,100}[\\\"'\\\\s]\" } Pipeline Architecture Below is the current architecture of your pipeline. After you have successfully configured the secrets scanning stage, you can proceed to the next module.","title":"Module 2: Secrets Scanning"},{"location":"02-secrets-scanning/#module-2-add-a-secrets-scanning-stage","text":"Time : 10 minutes Next, you need to setup a stage for identifying secrets throughout your code. For this stage you'll be leveraging trufflehog , a popular open source project for finding secrets accidentally committed in repositories. It essentially searches through git repositories for secrets, digging deep into commit history and branches. It identifies secrets by running entropy checks as well as high signal regex checks.","title":"Module 2 Add a secrets scanning stage"},{"location":"02-secrets-scanning/#create-the-build-spec-file","text":"Click on your Cloud9 IDE tab. In the left file tree, expand the configurations folder and open buildspec_secrets.yml . Review the YAML code below, paste it in the file, and save. version : 0.2 phases : pre_build : commands : - echo \"Setting CodeCommit Credentials\" - git config --global credential.helper '!aws codecommit credential-helper $@' - git config --global credential.UseHttpPath true - echo \"Copying secrets_config.json to the application directory\" - cp secrets_config.json $CODEBUILD_SRC_DIR_AppSource/secrets_config.json - echo \"Switching to the application directory\" - echo \"Installing truffleHog\" - which pip3 && pip3 --version - which python3 && python3 --version - pip3 install 'truffleHog>=2.1.0,<3.0' build : commands : - echo \"Build started on $(date)\" - echo \"Scanning with truffleHog...\" - trufflehog --regex --rules secrets_config.json --entropy=False \"$APP_REPO_URL\" post_build : commands : - echo \"Build completed on $(date)\"","title":"Create the Build Spec file"},{"location":"02-secrets-scanning/#add-the-trufflehog-regex-configuration","text":"When using trufflehog you can optionally specify a configuration file that contains custom regex checks. In the left file tree, expand the configurations folder and open secrets_config.json . Paste the JSON below and save the file. { \"Slack Token\" : \"(xox[p|b|o|a]-[0-9]{12}-[0-9]{12}-[0-9]{12}-[a-z0-9]{32})\" , \"RSA private key\" : \"-----BEGIN RSA PRIVATE KEY-----\" , \"SSH (OPENSSH) private key\" : \"-----BEGIN OPENSSH PRIVATE KEY-----\" , \"SSH (DSA) private key\" : \"-----BEGIN DSA PRIVATE KEY-----\" , \"SSH (EC) private key\" : \"-----BEGIN EC PRIVATE KEY-----\" , \"PGP private key block\" : \"-----BEGIN PGP PRIVATE KEY BLOCK-----\" , \"Facebook Oauth\" : \"[f|F][a|A][c|C][e|E][b|B][o|O][o|O][k|K].*['|\\\"][0-9a-f]{32}['|\\\"]\" , \"Twitter Oauth\" : \"[t|T][w|W][i|I][t|T][t|T][e|E][r|R].*['|\\\"][0-9a-zA-Z]{35,44}['|\\\"]\" , \"GitHub\" : \"[g|G][i|I][t|T][h|H][u|U][b|B].*['|\\\"][0-9a-zA-Z]{35,40}['|\\\"]\" , \"Google Oauth\" : \"(\\\"client_secret\\\":\\\"[a-zA-Z0-9-_]{24}\\\")\" , \"AWS API Key\" : \"AKIA[0-9A-Z]{16}\" , \"Heroku API Key\" : \"[h|H][e|E][r|R][o|O][k|K][u|U].*[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12}\" , \"Generic Secret\" : \"[s|S][e|E][c|C][r|R][e|E][t|T].*['|\\\"][0-9a-zA-Z]{32,45}['|\\\"]\" , \"Generic API Key\" : \"[a|A][p|P][i|I][_]?[k|K][e|E][y|Y].*['|\\\"][0-9a-zA-Z]{32,45}['|\\\"]\" , \"Slack Webhook\" : \"https://hooks.slack.com/services/T[a-zA-Z0-9_]{8}/B[a-zA-Z0-9_]{8}/[a-zA-Z0-9_]{24}\" , \"Google (GCP) Service-account\" : \"\\\"type\\\": \\\"service_account\\\"\" , \"Twilio API Key\" : \"SK[a-z0-9]{32}\" , \"Password in URL\" : \"[a-zA-Z]{3,10}://[^/\\\\s:@]{3,20}:[^/\\\\s:@]{3,20}@.{1,100}[\\\"'\\\\s]\" }","title":"Add the trufflehog regex configuration"},{"location":"02-secrets-scanning/#pipeline-architecture","text":"Below is the current architecture of your pipeline. After you have successfully configured the secrets scanning stage, you can proceed to the next module.","title":"Pipeline Architecture"},{"location":"03-vuln-scanning/","text":"Module 3 Add a vulnerability scanning stage Time : 10 minutes The last stage you will add will be for identifying vulnerabilities in your container image. For this stage you'll be using Anchore , a popular open source container compliance platform. This service can do a number of different validations but you will be primarily using it for checking your image for any Common Vulnerabilities and Exposures (CVE). Create the Build Spec file Click on your Cloud9 IDE tab. In the left file tree, expand the configurations folder and open buildspec_vuln.yml . Review the YAML code below, paste it in the file, and save. version : 0.2 phases : pre_build : commands : - apt-get update && apt-get install -y python-dev jq - docker pull anchore/engine-cli:v0.8.2 - curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py - python get-pip.py - pip install awscli - $(aws ecr get-login --no-include-email) - ANCHORE_CMD=\"docker run -e ANCHORE_CLI_URL=$ANCHORE_CLI_URL -e ANCHORE_CLI_USER=$ANCHORE_CLI_USER -e ANCHORE_CLI_PASS=$ANCHORE_CLI_PASS anchore/engine-cli:v0.8.2 anchore-cli\" - $ANCHORE_CMD registry add $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com awsauto awsauto --registry-type=awsecr || return 0 build : commands : - IMAGE=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME - docker build $CODEBUILD_SRC_DIR_AppSource -t $IMAGE - docker push $IMAGE post_build : commands : - $ANCHORE_CMD image add $IMAGE - while [ $($ANCHORE_CMD --json image get $IMAGE | jq -r '.[0].analysis_status') != \"analyzed\" ]; do sleep 1; done - $ANCHORE_CMD --json image vuln $IMAGE all > scan_results.json - jq -c --arg image $IMAGE --arg arn $IMAGE_ARN '. + {image_id:$image, image_arn:$arn}' scan_results.json >> tmp.json - mv tmp.json scan_results.json - aws lambda invoke --function-name $FUNCTION_ARN --invocation-type RequestResponse --payload file://scan_results.json outfile - cat scan_results.json | jq -r --arg threshold $FAIL_WHEN '.vulnerabilities[] | select(.severity==$threshold)' - if cat scan_results.json | jq -r --arg threshold $FAIL_WHEN '.vulnerabilities[] | (.severity==$threshold)' | grep -q true; then echo \"Vulnerabilties Found\" && exit 1; fi Commit all configuration changes Since you've made changes to a number of files in the configuration repo, you need to commit those changes to ensure your pipeline is pulling in the right files. cd /home/ec2-user/environment/configurations git add . git commit -m \"Updated Build Spec files and configurations.\" git push -u origin master Pipeline Architecture Below is the current architecture of your pipeline. After you have successfully configured the secrets scanning stage, you can proceed to the next module.","title":"Module 3: Vulnerability Scanning"},{"location":"03-vuln-scanning/#module-3-add-a-vulnerability-scanning-stage","text":"Time : 10 minutes The last stage you will add will be for identifying vulnerabilities in your container image. For this stage you'll be using Anchore , a popular open source container compliance platform. This service can do a number of different validations but you will be primarily using it for checking your image for any Common Vulnerabilities and Exposures (CVE).","title":"Module 3 Add a vulnerability scanning stage"},{"location":"03-vuln-scanning/#create-the-build-spec-file","text":"Click on your Cloud9 IDE tab. In the left file tree, expand the configurations folder and open buildspec_vuln.yml . Review the YAML code below, paste it in the file, and save. version : 0.2 phases : pre_build : commands : - apt-get update && apt-get install -y python-dev jq - docker pull anchore/engine-cli:v0.8.2 - curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py - python get-pip.py - pip install awscli - $(aws ecr get-login --no-include-email) - ANCHORE_CMD=\"docker run -e ANCHORE_CLI_URL=$ANCHORE_CLI_URL -e ANCHORE_CLI_USER=$ANCHORE_CLI_USER -e ANCHORE_CLI_PASS=$ANCHORE_CLI_PASS anchore/engine-cli:v0.8.2 anchore-cli\" - $ANCHORE_CMD registry add $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com awsauto awsauto --registry-type=awsecr || return 0 build : commands : - IMAGE=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME - docker build $CODEBUILD_SRC_DIR_AppSource -t $IMAGE - docker push $IMAGE post_build : commands : - $ANCHORE_CMD image add $IMAGE - while [ $($ANCHORE_CMD --json image get $IMAGE | jq -r '.[0].analysis_status') != \"analyzed\" ]; do sleep 1; done - $ANCHORE_CMD --json image vuln $IMAGE all > scan_results.json - jq -c --arg image $IMAGE --arg arn $IMAGE_ARN '. + {image_id:$image, image_arn:$arn}' scan_results.json >> tmp.json - mv tmp.json scan_results.json - aws lambda invoke --function-name $FUNCTION_ARN --invocation-type RequestResponse --payload file://scan_results.json outfile - cat scan_results.json | jq -r --arg threshold $FAIL_WHEN '.vulnerabilities[] | select(.severity==$threshold)' - if cat scan_results.json | jq -r --arg threshold $FAIL_WHEN '.vulnerabilities[] | (.severity==$threshold)' | grep -q true; then echo \"Vulnerabilties Found\" && exit 1; fi","title":"Create the Build Spec file"},{"location":"03-vuln-scanning/#commit-all-configuration-changes","text":"Since you've made changes to a number of files in the configuration repo, you need to commit those changes to ensure your pipeline is pulling in the right files. cd /home/ec2-user/environment/configurations git add . git commit -m \"Updated Build Spec files and configurations.\" git push -u origin master","title":"Commit all configuration changes"},{"location":"03-vuln-scanning/#pipeline-architecture","text":"Below is the current architecture of your pipeline. After you have successfully configured the secrets scanning stage, you can proceed to the next module.","title":"Pipeline Architecture"},{"location":"04-testing/","text":"Module 4 Pipeline Testing Time : 50 minutes Now that you have integrated multiple types of security testing into your pipeline you can test it to ensure your stages are effective in properly evaluating the security of your container-based applications. While going through each stage you will fix any misconfiguration or vulnerability so that your sample application is able to successfully pass through each stage and is pushed to AWS ECR. Pipeline Architecture Commit : Developer makes a commit to the Development branch. Pull Request : Developer makes a Pull Request Source Branch: Developement Destination Branch: Master Triggers Rule : A CloudWatch Event Rule is triggered based on the following events: pullRequestSourceBranchUpdated pullRequestCreated Invokes Function : The AWS Lambda Function is setup as a target for the CloudWatch Event Rule and it is invoked after the CloudWatch Event Rule is triggered Posts comment : The Lambda Function posts a comment to the Pull Request stating that the security testing is starting. Starts Pipeline : The Lambda Function starts the CodePipeline. Pull Request Stage : The stage pulls in these sources and stores them as artifacts in S3; CodeCommit repository: container-devsecops-wksp-app (development branch) CodeCommit repository: container-devsecops-wksp-config (master branch) Dockerfile Linting Stage : This stage pulls in the artifacts from S3 and uses Hadolint (build spec file and configuration file pulled in from S3) to lint the Dockerfile to ensure it adheres to best practices. Secrets Scanning Stage : This stage runs high signal regex checks directly against the CodeCommit Repository ( container-devsecops-wksp-app - development branch ) Vulnerability Scanning Stage : This stage builds the container image, pushes it to ECR, and triggers an Anchore vulnerability assessment against the image. If the scan results include any vulnerabilites that meet or exceed the threshold the build fails. If the vulnerabilities are lower than the set threshold, the CodeBuild project will invoke a Lambda function with the scan results as the payload. The Lambda function will then push the vulnerabilities into AWS Security Hub for future triaging, since the risk for those have been accepted. Publish Imaeg : This last stage builds the image using the destination commit hash as the tag and publishes it to AWS ECR. CodeBuild Triggers : If any CodeBuild Project fails a CloudWatch Event Rule is triggered. Triggers Lambda Function : The Lambda Function is setup as a target for the CloudWatch Event Rule and is invoked after the CloudWatch Event Rule is triggered. Adds Feedback to Pull Request : The Lambda Function takes the results from each stage and CodeBuild project and posts a comment back to the Pull Requst. This gives the developers fast feedback so they're able to fix any issues that are identified through the pipeline. Make a commit Now you can test your pipeline to see how your Pull Requests result with an image being built and pushed to AWS ECR. First, make a commit to the development branch of your sample application Within your Cloud9 IDE expand your sample application on the left side. Open the Dockerfile . Add a name to the Label line (Currently set to \"Sasquatch\"). Push your commit. cd /home/ec2-user/environment/sample-application git add Dockerfile git commit -m \"Modified Maintainer in Dockerfile\" git push -u origin development Create a Pull Request aws codecommit create-pull-request \\ --title \"Updated Maintainer\" \\ --description \"Please review these changes.\" \\ --targets repositoryName = container-devsecops-wksp-app,sourceReference = development,destinationReference = master Go to your AWS CodePipeline to view the progress and result. View the feedback loop Each time a stage is run the results of the CodeBuild Project are posted back to the Pull Request to act as a feedback loop for developers. Go to the CodeCommit console Click on the latest Pull Request. Click the Activity tab to view the feedback. Fix the Dockerfile linting defects In the feedback you should see multiple defects that were identified by the Dockerfile linting stage. The first defect can be fixed by modifying the hadolint configuration. Click on your Cloud9 IDE tab. In the left file tree, expand the configurations folder and open hadolint.yml . Fix the defect: Use of untrusted images DL3026 : Use only an allowed registry in the FROM image Description : Using externally provided images can result in the same types of risks that external software traditionally has, such as introducing malware, leaking data, or including components with vulnerabilities. To prevent the use of externally provided images you should only pull images from trusted registries. Fix* : Add - http://hub.docker.com/ under trustedRegistries***. Explanation : Since the image is pulling from Dockerhub we can include it on the list so that the build is able to pass. Adding Dockerhub is purely for testing purposes, in reality you would whitelist trusted registries that you host yourself or registries hosted by trusted 3rd parties. Reference : NIST SP 800-190: Application Container Security Guide - 3.1.5 Commit your configuration changes: cd /home/ec2-user/environment/configurations git add . git commit -m \"Added a trusted registry to hadolint configuration.\" git push -u origin master The next two defects can be fixed by modifying the Dockerfile. In the left file tree, expand the sample-application folder and open Dockerfile . Fix the defects: Image configuration defects DL3002 : Last USER should not be root. Description : To adhere the principals of least privileges, your containers should not be running as root. Most containerized processes are application services and therefore don\u2019t require root access. Fix : Change USER to a non privileged user. Add the following to the Dockerfile underneath RUN apk : RUN adduser sasquatch -D Next, replace USER root with USER sasquatch Explanation : By creating a user in your Dockerfile, you are making it not only secure by default but also easier to keep secure. Reference : NIST SP 800-190: Application Container Security Guide - 3.1.2 Image configuration defects DL3007 : Using latest is prone to errors if the image will ever update. Description : Latest is just a tag you add to an image and is no way dynamic. It can be overwritten and prove difficult to understand what version of the software is installed. Using the latest tag can effect the availability of your application and you should look at using a more explicit tag. Fix : Pin the version explicitly to a release tag. Replace FROM python:latest with FROM python:3.7-alpine Reference : NIST SP 800-190: Application Container Security Guide - 3.1.2 Commit your application source code changes: cd /home/ec2-user/environment/sample-application git add Dockerfile git commit -m \"Fixed Dockerfile linting issues.\" git push -u origin development Go to your AWS CodePipeline to view the progress and result. View the Pull Request Feedback Updating the Pull Request branch automatically triggers the pipeline again. You can view the feedback to see if the defects were remediated. Go to the CodeCommit console Click on the latest Pull Request. Click the Activity tab to view the feedback. Remove secrets Based on the feedback you received in the Pull request you can see that secrets were identified in your code. Click on Logs in the comment or view the CodeBuild Project history to identify the secret and the file it is located in. How could you improve the feedback loop to make it easier for the developer? Remove the secret from the file. Modify trufflehog configuration Currently your trufflehog configuration is scanning through all of your commits to identify secrets. Since you'll be removing any current secrets you can modify the configuration to only scan new commits to speed up the build. In the left file tree, expand the configurations folder and open buildspec_secrets.yml . Update your trufflehog configuration to only scan 1 commit deep. Change this command: - trufflehog --regex --rules secrets_config.json --entropy=False \"$APP_REPO_URL\" To this: - trufflehog --regex --rules secrets_config.json --entropy = False --max_depth 1 \" $APP_REPO_URL \" The second line adds \"--max-depth 1\" which limits the scan depth. Commit your configuration changes: cd /home/ec2-user/environment/configurations git add . git commit -m \"Modifed max-depth in trufflehog command.\" git push -u origin master Commit your application changes: cd /home/ec2-user/environment/sample-application git add . git commit -m \"Removed access key.\" git push -u origin development Go to your AWS CodePipeline to view the progress and result. View the Pull Request Feedback Updating the Pull Request branch automatically triggers the pipeline again. You can view the feedback to see if the defects were remediated. Go to the CodeCommit console Click on the latest Pull Request. Click the Activity tab to view the feedback. You'll notice that your pipeline is still failing on the secrets stage. If you look at the commit that's being scanned you'll see that the access key still exists in that commit because it is part of the diff. Make one more commit and you'll see that your build passes the secret scanning stage successfully. In the left file tree, expand the sample-application folder and open Dockerfile . Change the Label to the following: LABEL maintainer = \"Sasquatch\" version = \"1.0\" Commit your application source code changes: cd /home/ec2-user/environment/sample-application git add Dockerfile git commit -m \"Added version Label.\" git push -u origin development Embedded clear text secrets Description : Many applications require secrets to enable communication with other serivces or backend components. When an application is packaged into an image, these secrets can be embedded directly into the image. This creates a security risk in which anyone with access to the image can easily obtain the secrets. Fix : Remove secrets from source code and leverage a secure solution for managing secrets like AWS Secrets Manager or AWS Systems Manager Parameter Store . Explanation : Best practice is to the remove secrets from all previous commits and rotate any credential found but due to time constraints you removed the secret and modified the scanning tool to only scan new commits. Reference : NIST SP 800-190: Application Container Security Guide - 3.1.4 View the Pull Request Feedback Updating the Pull Request branch automatically triggers the pipeline again. You can view the feedback to see if the defects were remediated. Go to the CodeCommit console Click on the latest Pull Request. Click the Activity tab to view the feedback. View vulnerabilities In the feedback you should see information regarding any vulnerabilities that were found in the image. When you went through the environment setup you specified a vulnerability threshold of \"High\" , which means that the build will fail if an image contains any High or Critical vulnerabilities. Specifying a threshold allows you to put in a place a risk tolerance for different severities of vulnerabilities. This allows your developers to continue to move quickly with low risk vulnerabilities that can be triaged and fixed later on. In the current setup, all vulnerabilities below the threshold will be pushed to AWS Security Hub. Since the build fails the vulnerability analysis stage we need to fix the issue with so that the image does not contain any \"High\" rated vulnerabilitiy. Go to the Security Hub console. Click Findings in the left navigation Click the search field and select a filter of Product Name EQUALS Default . The resulting findings are all of the vulnerabilities found in the image. Click on the HIGH rated vulnerability and check the Description for the vulnerability reported. E.g. Follow the URL in the Source URL to see additional information about the reported vulnerability. Image vulnerabilities Description : Images are static archive files that include all of the components used to run an application. Components within an image may be missing critical security updates or be outdated which can lead to exploitation and unauthorized system access. Fix : Containers should be looked at as immutable and as such shouldn't be patched directly. Instead the vulernabilities should be fixed upstream in the source code and configuration of the image and then the image should be rebuilt and published. This ensures that all new containers instantiated from the image don't include the vulnerabilities. Update the affected package in Dockerfile: Following the recommended Remediation update the flask-cors version to 3.0.9 in the Dockerfile. flask-cors <= 3.0.9 Reference : NIST SP 800-190: Application Container Security Guide - 3.1.1 Commit your application source code changes: cd /home/ec2-user/environment/sample-application git add Dockerfile git commit -m \"Update flask-cors version to fix CVE-2020-25032\" git push -u origin development View the CodePipeline Updating the Pull Request branch automatically triggers the pipeline again. This time go to the pipeline to view your code progress through the security testing. Go to the CodePipeline console Scroll down as your code progresses through each stage. View Image The last stage of the pipeline builds the image, publishes it to AWS ECR, and then merges athe Pull Request. You'll see in the feedback that a message is posted regarding the outcome of this stage and you'll notice that the Pull Request has been merged. Go to the CodeCommit console Click on the latest Pull Request. You may need to change the filter to \"Closed Pull Requests\" since the last stage publishes the image, merges the code, and closes the PR. Click the Activity tab to view the feedback. Click the AWS ECR repository link. Following tagging best practices, your image is tagged with the Git commit hash of your application. Congratulations! You've completed the Integrating security into your container pipeline workshop. You now have a CI/CD pipeline for securely building and publishing your container-based applications. You can proceed to the next module to clean up the resources in your account.","title":"Module 4: Pipeline Testing"},{"location":"04-testing/#module-4-pipeline-testing","text":"Time : 50 minutes Now that you have integrated multiple types of security testing into your pipeline you can test it to ensure your stages are effective in properly evaluating the security of your container-based applications. While going through each stage you will fix any misconfiguration or vulnerability so that your sample application is able to successfully pass through each stage and is pushed to AWS ECR.","title":"Module 4 Pipeline Testing"},{"location":"04-testing/#pipeline-architecture","text":"Commit : Developer makes a commit to the Development branch. Pull Request : Developer makes a Pull Request Source Branch: Developement Destination Branch: Master Triggers Rule : A CloudWatch Event Rule is triggered based on the following events: pullRequestSourceBranchUpdated pullRequestCreated Invokes Function : The AWS Lambda Function is setup as a target for the CloudWatch Event Rule and it is invoked after the CloudWatch Event Rule is triggered Posts comment : The Lambda Function posts a comment to the Pull Request stating that the security testing is starting. Starts Pipeline : The Lambda Function starts the CodePipeline. Pull Request Stage : The stage pulls in these sources and stores them as artifacts in S3; CodeCommit repository: container-devsecops-wksp-app (development branch) CodeCommit repository: container-devsecops-wksp-config (master branch) Dockerfile Linting Stage : This stage pulls in the artifacts from S3 and uses Hadolint (build spec file and configuration file pulled in from S3) to lint the Dockerfile to ensure it adheres to best practices. Secrets Scanning Stage : This stage runs high signal regex checks directly against the CodeCommit Repository ( container-devsecops-wksp-app - development branch ) Vulnerability Scanning Stage : This stage builds the container image, pushes it to ECR, and triggers an Anchore vulnerability assessment against the image. If the scan results include any vulnerabilites that meet or exceed the threshold the build fails. If the vulnerabilities are lower than the set threshold, the CodeBuild project will invoke a Lambda function with the scan results as the payload. The Lambda function will then push the vulnerabilities into AWS Security Hub for future triaging, since the risk for those have been accepted. Publish Imaeg : This last stage builds the image using the destination commit hash as the tag and publishes it to AWS ECR. CodeBuild Triggers : If any CodeBuild Project fails a CloudWatch Event Rule is triggered. Triggers Lambda Function : The Lambda Function is setup as a target for the CloudWatch Event Rule and is invoked after the CloudWatch Event Rule is triggered. Adds Feedback to Pull Request : The Lambda Function takes the results from each stage and CodeBuild project and posts a comment back to the Pull Requst. This gives the developers fast feedback so they're able to fix any issues that are identified through the pipeline.","title":"Pipeline Architecture"},{"location":"04-testing/#make-a-commit","text":"Now you can test your pipeline to see how your Pull Requests result with an image being built and pushed to AWS ECR. First, make a commit to the development branch of your sample application Within your Cloud9 IDE expand your sample application on the left side. Open the Dockerfile . Add a name to the Label line (Currently set to \"Sasquatch\"). Push your commit. cd /home/ec2-user/environment/sample-application git add Dockerfile git commit -m \"Modified Maintainer in Dockerfile\" git push -u origin development","title":"Make a commit"},{"location":"04-testing/#create-a-pull-request","text":"aws codecommit create-pull-request \\ --title \"Updated Maintainer\" \\ --description \"Please review these changes.\" \\ --targets repositoryName = container-devsecops-wksp-app,sourceReference = development,destinationReference = master Go to your AWS CodePipeline to view the progress and result.","title":"Create a Pull Request"},{"location":"04-testing/#view-the-feedback-loop","text":"Each time a stage is run the results of the CodeBuild Project are posted back to the Pull Request to act as a feedback loop for developers. Go to the CodeCommit console Click on the latest Pull Request. Click the Activity tab to view the feedback.","title":"View the feedback loop"},{"location":"04-testing/#fix-the-dockerfile-linting-defects","text":"In the feedback you should see multiple defects that were identified by the Dockerfile linting stage. The first defect can be fixed by modifying the hadolint configuration. Click on your Cloud9 IDE tab. In the left file tree, expand the configurations folder and open hadolint.yml . Fix the defect: Use of untrusted images DL3026 : Use only an allowed registry in the FROM image Description : Using externally provided images can result in the same types of risks that external software traditionally has, such as introducing malware, leaking data, or including components with vulnerabilities. To prevent the use of externally provided images you should only pull images from trusted registries. Fix* : Add - http://hub.docker.com/ under trustedRegistries***. Explanation : Since the image is pulling from Dockerhub we can include it on the list so that the build is able to pass. Adding Dockerhub is purely for testing purposes, in reality you would whitelist trusted registries that you host yourself or registries hosted by trusted 3rd parties. Reference : NIST SP 800-190: Application Container Security Guide - 3.1.5 Commit your configuration changes: cd /home/ec2-user/environment/configurations git add . git commit -m \"Added a trusted registry to hadolint configuration.\" git push -u origin master The next two defects can be fixed by modifying the Dockerfile. In the left file tree, expand the sample-application folder and open Dockerfile . Fix the defects: Image configuration defects DL3002 : Last USER should not be root. Description : To adhere the principals of least privileges, your containers should not be running as root. Most containerized processes are application services and therefore don\u2019t require root access. Fix : Change USER to a non privileged user. Add the following to the Dockerfile underneath RUN apk : RUN adduser sasquatch -D Next, replace USER root with USER sasquatch Explanation : By creating a user in your Dockerfile, you are making it not only secure by default but also easier to keep secure. Reference : NIST SP 800-190: Application Container Security Guide - 3.1.2 Image configuration defects DL3007 : Using latest is prone to errors if the image will ever update. Description : Latest is just a tag you add to an image and is no way dynamic. It can be overwritten and prove difficult to understand what version of the software is installed. Using the latest tag can effect the availability of your application and you should look at using a more explicit tag. Fix : Pin the version explicitly to a release tag. Replace FROM python:latest with FROM python:3.7-alpine Reference : NIST SP 800-190: Application Container Security Guide - 3.1.2 Commit your application source code changes: cd /home/ec2-user/environment/sample-application git add Dockerfile git commit -m \"Fixed Dockerfile linting issues.\" git push -u origin development Go to your AWS CodePipeline to view the progress and result. View the Pull Request Feedback Updating the Pull Request branch automatically triggers the pipeline again. You can view the feedback to see if the defects were remediated. Go to the CodeCommit console Click on the latest Pull Request. Click the Activity tab to view the feedback.","title":"Fix the Dockerfile linting defects"},{"location":"04-testing/#remove-secrets","text":"Based on the feedback you received in the Pull request you can see that secrets were identified in your code. Click on Logs in the comment or view the CodeBuild Project history to identify the secret and the file it is located in. How could you improve the feedback loop to make it easier for the developer? Remove the secret from the file. Modify trufflehog configuration Currently your trufflehog configuration is scanning through all of your commits to identify secrets. Since you'll be removing any current secrets you can modify the configuration to only scan new commits to speed up the build. In the left file tree, expand the configurations folder and open buildspec_secrets.yml . Update your trufflehog configuration to only scan 1 commit deep. Change this command: - trufflehog --regex --rules secrets_config.json --entropy=False \"$APP_REPO_URL\" To this: - trufflehog --regex --rules secrets_config.json --entropy = False --max_depth 1 \" $APP_REPO_URL \" The second line adds \"--max-depth 1\" which limits the scan depth. Commit your configuration changes: cd /home/ec2-user/environment/configurations git add . git commit -m \"Modifed max-depth in trufflehog command.\" git push -u origin master Commit your application changes: cd /home/ec2-user/environment/sample-application git add . git commit -m \"Removed access key.\" git push -u origin development Go to your AWS CodePipeline to view the progress and result. View the Pull Request Feedback Updating the Pull Request branch automatically triggers the pipeline again. You can view the feedback to see if the defects were remediated. Go to the CodeCommit console Click on the latest Pull Request. Click the Activity tab to view the feedback. You'll notice that your pipeline is still failing on the secrets stage. If you look at the commit that's being scanned you'll see that the access key still exists in that commit because it is part of the diff. Make one more commit and you'll see that your build passes the secret scanning stage successfully. In the left file tree, expand the sample-application folder and open Dockerfile . Change the Label to the following: LABEL maintainer = \"Sasquatch\" version = \"1.0\" Commit your application source code changes: cd /home/ec2-user/environment/sample-application git add Dockerfile git commit -m \"Added version Label.\" git push -u origin development Embedded clear text secrets Description : Many applications require secrets to enable communication with other serivces or backend components. When an application is packaged into an image, these secrets can be embedded directly into the image. This creates a security risk in which anyone with access to the image can easily obtain the secrets. Fix : Remove secrets from source code and leverage a secure solution for managing secrets like AWS Secrets Manager or AWS Systems Manager Parameter Store . Explanation : Best practice is to the remove secrets from all previous commits and rotate any credential found but due to time constraints you removed the secret and modified the scanning tool to only scan new commits. Reference : NIST SP 800-190: Application Container Security Guide - 3.1.4 View the Pull Request Feedback Updating the Pull Request branch automatically triggers the pipeline again. You can view the feedback to see if the defects were remediated. Go to the CodeCommit console Click on the latest Pull Request. Click the Activity tab to view the feedback.","title":"Remove secrets"},{"location":"04-testing/#view-vulnerabilities","text":"In the feedback you should see information regarding any vulnerabilities that were found in the image. When you went through the environment setup you specified a vulnerability threshold of \"High\" , which means that the build will fail if an image contains any High or Critical vulnerabilities. Specifying a threshold allows you to put in a place a risk tolerance for different severities of vulnerabilities. This allows your developers to continue to move quickly with low risk vulnerabilities that can be triaged and fixed later on. In the current setup, all vulnerabilities below the threshold will be pushed to AWS Security Hub. Since the build fails the vulnerability analysis stage we need to fix the issue with so that the image does not contain any \"High\" rated vulnerabilitiy. Go to the Security Hub console. Click Findings in the left navigation Click the search field and select a filter of Product Name EQUALS Default . The resulting findings are all of the vulnerabilities found in the image. Click on the HIGH rated vulnerability and check the Description for the vulnerability reported. E.g. Follow the URL in the Source URL to see additional information about the reported vulnerability. Image vulnerabilities Description : Images are static archive files that include all of the components used to run an application. Components within an image may be missing critical security updates or be outdated which can lead to exploitation and unauthorized system access. Fix : Containers should be looked at as immutable and as such shouldn't be patched directly. Instead the vulernabilities should be fixed upstream in the source code and configuration of the image and then the image should be rebuilt and published. This ensures that all new containers instantiated from the image don't include the vulnerabilities. Update the affected package in Dockerfile: Following the recommended Remediation update the flask-cors version to 3.0.9 in the Dockerfile. flask-cors <= 3.0.9 Reference : NIST SP 800-190: Application Container Security Guide - 3.1.1 Commit your application source code changes: cd /home/ec2-user/environment/sample-application git add Dockerfile git commit -m \"Update flask-cors version to fix CVE-2020-25032\" git push -u origin development View the CodePipeline Updating the Pull Request branch automatically triggers the pipeline again. This time go to the pipeline to view your code progress through the security testing. Go to the CodePipeline console Scroll down as your code progresses through each stage.","title":"View vulnerabilities"},{"location":"04-testing/#view-image","text":"The last stage of the pipeline builds the image, publishes it to AWS ECR, and then merges athe Pull Request. You'll see in the feedback that a message is posted regarding the outcome of this stage and you'll notice that the Pull Request has been merged. Go to the CodeCommit console Click on the latest Pull Request. You may need to change the filter to \"Closed Pull Requests\" since the last stage publishes the image, merges the code, and closes the PR. Click the Activity tab to view the feedback. Click the AWS ECR repository link. Following tagging best practices, your image is tagged with the Git commit hash of your application. Congratulations! You've completed the Integrating security into your container pipeline workshop. You now have a CI/CD pipeline for securely building and publishing your container-based applications. You can proceed to the next module to clean up the resources in your account.","title":"View Image"},{"location":"05-cleanup/","text":"Module 5: Cleanup In order to prevent charges to your account we recommend cleaning up the infrastructure that was created. If you plan to keep things running so you can examine the workshop a bit more please remember to do the cleanup when you are done. It is very easy to leave things running in an AWS account, forget about it, and then accrue charges. You will need to manually delete some resources before you delete the CloudFormation stacks so please do the following steps in order. Open your Cloud9 IDE Run the following: # Get Account # account = ` aws sts get - caller - identity --query [Account] --output text` # Delete S3 Bucket aws s3 rm s3 : // container - devsecops - wksp - $ account - us - east - 2 - artifacts --recursive aws s3api delete - bucket --bucket container-devsecops-wksp-$account-us-east-2-artifacts aws logs delete - log - group --log-group-name /aws/codebuild/container-devsecops-wksp-build-dockerfile aws logs delete - log - group --log-group-name /aws/codebuild/container-devsecops-wksp-build-secrets aws logs delete - log - group --log-group-name /aws/codebuild/container-devsecops-wksp-scan-image aws logs delete - log - group --log-group-name /aws/codebuild/container-devsecops-wksp-publish aws logs delete - log - group --log-group-name /aws/codebuild/container-devsecops-wksp-anchore-build aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-initial-commit aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-pr aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-codebuild-dockerfile aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-codebuild-secrets aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-codebuild-vulnerability aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-codebuild-publish # Delete ECR Repositories aws ecr delete - repository --repository-name container-devsecops-wksp-sample --force aws ecr delete - repository --repository-name container-devsecops-wksp-anchore --force aws ecr delete - repository --repository-name container-devsecops-wksp-scratch --force # Delete CloudFormation Stacks aws cloudformation delete - stack --stack-name container-dso-wksp echo 'Completed cleanup.'","title":"Module 5: Cleanup"},{"location":"05-cleanup/#module-5-cleanup","text":"In order to prevent charges to your account we recommend cleaning up the infrastructure that was created. If you plan to keep things running so you can examine the workshop a bit more please remember to do the cleanup when you are done. It is very easy to leave things running in an AWS account, forget about it, and then accrue charges. You will need to manually delete some resources before you delete the CloudFormation stacks so please do the following steps in order. Open your Cloud9 IDE Run the following: # Get Account # account = ` aws sts get - caller - identity --query [Account] --output text` # Delete S3 Bucket aws s3 rm s3 : // container - devsecops - wksp - $ account - us - east - 2 - artifacts --recursive aws s3api delete - bucket --bucket container-devsecops-wksp-$account-us-east-2-artifacts aws logs delete - log - group --log-group-name /aws/codebuild/container-devsecops-wksp-build-dockerfile aws logs delete - log - group --log-group-name /aws/codebuild/container-devsecops-wksp-build-secrets aws logs delete - log - group --log-group-name /aws/codebuild/container-devsecops-wksp-scan-image aws logs delete - log - group --log-group-name /aws/codebuild/container-devsecops-wksp-publish aws logs delete - log - group --log-group-name /aws/codebuild/container-devsecops-wksp-anchore-build aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-initial-commit aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-pr aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-codebuild-dockerfile aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-codebuild-secrets aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-codebuild-vulnerability aws logs delete - log - group --log-group-name /aws/lambda/container-devsecops-wksp-codebuild-publish # Delete ECR Repositories aws ecr delete - repository --repository-name container-devsecops-wksp-sample --force aws ecr delete - repository --repository-name container-devsecops-wksp-anchore --force aws ecr delete - repository --repository-name container-devsecops-wksp-scratch --force # Delete CloudFormation Stacks aws cloudformation delete - stack --stack-name container-dso-wksp echo 'Completed cleanup.'","title":"Module 5: Cleanup"},{"location":"contribute/","text":"Contributing Guidelines Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Reporting Bugs/Feature Requests We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment Contributing via Pull Requests Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the master branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request . Finding contributions to work on Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels ((enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start. Code of Conduct This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments. Security issue notifications If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue. Licensing See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Contributing"},{"location":"contribute/#contributing-guidelines","text":"Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.","title":"Contributing Guidelines"},{"location":"contribute/#reporting-bugsfeature-requests","text":"We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment","title":"Reporting Bugs/Feature Requests"},{"location":"contribute/#contributing-via-pull-requests","text":"Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the master branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request .","title":"Contributing via Pull Requests"},{"location":"contribute/#finding-contributions-to-work-on","text":"Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels ((enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.","title":"Finding contributions to work on"},{"location":"contribute/#code-of-conduct","text":"This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.","title":"Code of Conduct"},{"location":"contribute/#security-issue-notifications","text":"If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue.","title":"Security issue notifications"},{"location":"contribute/#licensing","text":"See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Licensing"},{"location":"license/","text":"License MIT License Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"MIT License Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"}]}